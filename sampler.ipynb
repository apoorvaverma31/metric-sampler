{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "import numpy as np\n",
    "import utils\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_imb_data(max_num, class_num, gamma, inv=False, bal=False):\n",
    "    print('max_num={}'.format(max_num))\n",
    "    mu = np.power(1/gamma, 1/(class_num - 1))\n",
    "    class_num_list = []\n",
    "    for i in range(class_num):\n",
    "        if(inv):\n",
    "            class_num_list.append(int(max_num * np.power(mu, class_num-i-1)))\n",
    "        else:\n",
    "            class_num_list.append(int(max_num * np.power(mu, i)))\n",
    "    if(bal):\n",
    "        per_class = sum(class_num_list)/class_num\n",
    "        class_num_list = [int(per_class) for i in range(class_num)]\n",
    "    print(class_num_list)\n",
    "    return list(class_num_list)\n",
    "\n",
    "N_SAMPLES_PER_CLASS = make_imb_data(1250, 10, 100 ,False, False)\n",
    "U_SAMPLES_PER_CLASS = make_imb_data(3 * 1250, 10, 100, False, False)\n",
    "N_SAMPLES_PER_CLASS_T = torch.Tensor(N_SAMPLES_PER_CLASS)\n",
    "\n",
    "train_labeled_set, train_unlabeled_set, test_set = utils.get_cifar10('/home/apoorva/Datasets', N_SAMPLES_PER_CLASS\n",
    ", U_SAMPLES_PER_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_dist = torch.nn.functional.normalize(torch.ones([10,10]))\n",
    "# target_dist = torch.diag(torch.ones(10))\n",
    "target_dist = torch.zeros([10, 10])\n",
    "for i in range(10):\n",
    "    target_dist[i][0] = 1\n",
    "labeled_loader = torch.utils.data.DataLoader(train_labeled_set, batch_size=128, num_workers=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_labels = [train_unlabeled_set.targets[i] for i in np.arange(len(train_unlabeled_set.targets))]\n",
    "\n",
    "def get_weights(pseudo_labels, lbl, target_dist):\n",
    "    \"\"\"Returns the sampling weights for each data instance in the dataset \n",
    "\n",
    "    Args:\n",
    "        pseudo_labels (list): list of pseudo-labels generated by the model\n",
    "        lbl (tensor): labels present in one batch obtained from the dataloader\n",
    "        target_dist (tensor): target distribution obtained by dM/dCij\n",
    "\n",
    "    Returns:\n",
    "        list: list of instance-wise weights to be passed into the sampler\n",
    "    \"\"\"\n",
    "\n",
    "    wts = []\n",
    "    for t in pseudo_labels:\n",
    "        wts.append(float(target_dist[lbl][t])/N_SAMPLES_PER_CLASS[t])\n",
    "    return wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(counter_class):\n",
    "    names = list(counter_class.keys())\n",
    "    values = list(counter_class.values())\n",
    "\n",
    "    plt.bar(names, values, tick_label=names)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(unlabeled_train, num_classes, pseudo_labels, target_dist):\n",
    "    \"\"\"\n",
    "    Returns a dict of (num_classes) dataloaders of batch size 1 depending on the target distribution\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): number of dataloaders (equal to number of classes in the dataset)\n",
    "        pseudo_labels (list): list of pseudo-labels generated by the model\n",
    "        target_dist (tensor): target distribution obtained by dM/dCij\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary of num_classes loaders\n",
    "    \"\"\"\n",
    "    loader_dict = {}\n",
    "    for i in range(num_classes):\n",
    "        pl_weights = get_weights(pseudo_labels, i, target_dist)\n",
    "        sampler = torch.utils.data.WeightedRandomSampler(weights= pl_weights, num_samples = len(pseudo_labels), replacement = True)\n",
    "        loader = torch.utils.data.DataLoader(unlabeled_train, batch_size=None, num_workers=8, sampler=sampler)\n",
    "        loader_dict.update({f\"{i}\":loader})\n",
    "    return loader_dict\n",
    "    \n",
    "\n",
    "# loaders = get_loaders(train_unlabeled_set, 10, pseudo_labels, target_dist)\n",
    "# for i in range(10):\n",
    "#     ldr = loaders[f'{i}']\n",
    "#     labels = [int(i) for _, i, _ in ldr]\n",
    "#     print(Counter(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnlabeledDatasetCollate(Dataset):\n",
    "    \"\"\"\n",
    "    Class for collating the pseudo label-image pairs after target-distribution dependent sampling\n",
    "    Returns a dataset object that can be passed into a dataloader directly\n",
    "    \"\"\"\n",
    "    def __init__(self, labels_list, images_list, transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels_list (list): list of labels obtained after sampling\n",
    "            images_list (list): list of images corresponding to the labels sampled\n",
    "            transform, target_transform (torch.nn.Sequential, optional): if we intend to apply any transforms on \n",
    "            the sampled images. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.labels_list = labels_list\n",
    "        self.images_list = images_list\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_list) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images_list[idx]\n",
    "        label = self.labels_list[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "list_dist = np.zeros(10)\n",
    "for imgs, lbls, idxs in labeled_loader:    \n",
    "    sl_list = []\n",
    "    image_list = []\n",
    "    imgs = imgs.to(device)\n",
    "    lbls = lbls.to(device)  \n",
    "    loaders = get_loaders(train_unlabeled_set, 10, pseudo_labels, target_dist) # todo: train_unlabeled_set replaced by a megabatch\n",
    "    for i in lbls:\n",
    "        image , sampled_label, _ = next(iter(loaders[f'{i}']))\n",
    "        sl_list.append(int(sampled_label)) \n",
    "        image_list.append(image)\n",
    "    # print(Counter(sl_list))\n",
    "    ulb_dataset_sampled = UnlabeledDatasetCollate(sl_list, image_list)\n",
    "    ulb_loader = torch.utils.data.DataLoader(ulb_dataset_sampled, batch_size=128, num_workers=8, shuffle=True)\n",
    "    img, lbl_ulb = next(iter(ulb_loader))\n",
    "    lbl_ulb = lbl_ulb.numpy()\n",
    "    for i in lbl_ulb:\n",
    "        list_dist[i]+=1\n",
    "    print(Counter(lbl_ulb))\n",
    "    plot_distribution(Counter(lbl_ulb))\n",
    "\n",
    "print(list_dist) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnlabeledDataLoader():\n",
    "\n",
    "    def __init__(self, ulb_dataset, model, target_dist, batch_size):\n",
    "        super(UnlabeledDataLoader, self).__init__()\n",
    "        self.model = model\n",
    "        self.ulb_dataset = ulb_dataset\n",
    "        self.target_dist = target_dist\n",
    "        self.batch_size = batch_size\n",
    "        self.pseudo_labels = self.gen_pl()\n",
    "    \n",
    "    def gen_pl(self):\n",
    "        list_labels = []\n",
    "        ulb_loader = torch.utils.data.DataLoader(self.ulb_dataset, batch_size= 128, num_workers=8, shuffle=True)\n",
    "        for img in ulb_loader:\n",
    "            list_labels.append(self.model(img).numpy().flatten())\n",
    "        return list_labels\n",
    "            \n",
    "    def get_loaders(self):\n",
    "        \"\"\"\n",
    "        Returns a dict of (num_classes) dataloaders of batch size 1 depending on the target distribution\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): number of dataloaders (equal to number of classes in the dataset)\n",
    "            pseudo_labels (list): list of pseudo-labels generated by the model\n",
    "            target_dist (tensor): target distribution obtained by dM/dCij\n",
    "\n",
    "        Returns:\n",
    "            dict: dictionary of num_classes loaders\n",
    "        \"\"\"\n",
    "        loader_dict = {}\n",
    "        for i in range(target_dist.shape[0]):\n",
    "            pl_weights = get_weights(self.pseudo_labels, i, self.target_dist)\n",
    "            sampler = torch.utils.data.WeightedRandomSampler(weights= pl_weights, num_samples = len(self.pseudo_labels), replacement = True)\n",
    "            loader = torch.utils.data.DataLoader(self.ulb_dataset, batch_size=None, num_workers=8, sampler=sampler)\n",
    "            loader_dict.update({f\"{i}\":loader})\n",
    "        return loader_dict\n",
    "\n",
    "        \n",
    "\n",
    "    def get_weights(self, lbl):\n",
    "        \"\"\"Returns the sampling weights for each data instance in the dataset \n",
    "\n",
    "        Args:\n",
    "            pseudo_labels (list): list of pseudo-labels generated by the model\n",
    "            lbl (tensor): labels present in one batch obtained from the dataloader\n",
    "            target_dist (tensor): target distribution obtained by dM/dCij\n",
    "\n",
    "        Returns:\n",
    "            list: list of instance-wise weights to be passed into the sampler\n",
    "        \"\"\"\n",
    "\n",
    "        wts = []\n",
    "        for t in self.pseudo_labels:\n",
    "            wts.append(float(self.target_dist[lbl][t])/N_SAMPLES_PER_CLASS[t])\n",
    "        return wts\n",
    "\n",
    "    def get_batch(self, labels):\n",
    "        \"\"\"Returns a batch of unlabeled images and their pseudo labels sampled from a distribution dependent \n",
    "\n",
    "        Args:\n",
    "            labels (torch.tensor): labels obtained from one minibatch of the labeled loader\n",
    "\n",
    "        Returns:\n",
    "            img: torch.tensor of unlabeled images (shape NxCxHxW)\n",
    "            lbl_ulb: torch tensor of sampled pseudo labels (shape Nx1) \n",
    "        \"\"\"\n",
    "        sl_list = []\n",
    "        image_list = []\n",
    "        loaders = self.get_loaders(self.ulb_dataset, len(self.ulb_dataset.classes), self.pseudo_labels, self.target_dist)\n",
    "        for i in labels:\n",
    "            image , sampled_label, _ = next(iter(loaders[f'{i}']))\n",
    "            sl_list.append(int(sampled_label)) \n",
    "            image_list.append(image)\n",
    "        ulb_dataset_sampled = UnlabeledDatasetCollate(sl_list, image_list)\n",
    "        ulb_loader = torch.utils.data.DataLoader(ulb_dataset_sampled, batch_size=128, num_workers=8, shuffle=True)\n",
    "        img, lbl_ulb = next(iter(ulb_loader))\n",
    "        return img, lbl_ulb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8dd4fa52c2be142a1b05e1fd214df78dd67bbee7dce6644d036e63483141898"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
